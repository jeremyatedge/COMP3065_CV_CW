# -*- coding: utf-8 -*-
"""
    Description:
        Own implemented user interface for optical flow tracking, some basic functions are:
            1. Choose input video file.
            2. Play or pause the video to view specific frame (Like a video player).
            3. Toggle the tracks.
        Some functions are generated by PyQt5 UI code generator 5.15.4.
    Author:
        Jingjin Li (20127165)
    Date:
        2022.05.04
"""
import cv2
import sys
import opticalFlow
import numpy as np
from PyQt5 import QtCore, QtGui, QtWidgets
from PyQt5.QtCore import QTimer, Qt
from PyQt5.QtGui import QImage, QPixmap
from PyQt5.QtWidgets import QFileDialog

import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--track_length', help="The length of the tracking line.", default='Infinite')
parser.add_argument('--detect_interval', help="The interval for re-detect the good features of the image.",
                    type=int, default=20)
parser.add_argument('--window_size', help="Optical flow detected window size.", type=int, default=5)
parser.add_argument('--pyramid_level', help="Pyramid level in optical flow.", type=int, default=5)
parser.add_argument('--iters', help="The calculate iteration of optical flow.", type=int, default=10)
args = parser.parse_args()

# Init global parameters
invoke_sec = 1
resize_dim = 580
track_length = args.track_length
detect_interval = args.detect_interval

# Parameters for Shi-Tomasi corner detection.
feature_params = dict(maxCorners=10, qualityLevel=0.1, minDistance=15, blockSize=3)


class Ui_MainWindow(object):
    def __init__(self, MainWindow):
        MainWindow.setObjectName("MainWindow")
        MainWindow.setFixedSize(620, 410)

        self.central_widget = QtWidgets.QWidget(MainWindow)
        self.label = QtWidgets.QLabel(self.central_widget)
        self.label_info = QtWidgets.QLabel(self.central_widget)

        self.pushButton_file = QtWidgets.QPushButton(self.central_widget)
        self.pushButton_play_pause = QtWidgets.QPushButton(self.central_widget)
        self.pushButton_toggle_track = QtWidgets.QPushButton(self.central_widget)

        self.timer_camera = QTimer()
        self.play_status = False
        self.file_status = False
        self.toggle_track = True
        self.previousGray = None
        self.frame = None
        self.cap = None
        self.tracks = None
        self.mask = None
        self.scale = None
        self.frame_index = None
        self.setupUi()
        self.setupEvents()

        MainWindow.setCentralWidget(self.central_widget)
        self.retranslateUi(MainWindow)
        QtCore.QMetaObject.connectSlotsByName(MainWindow)

    # Set up related tools on UI.
    def setupUi(self):
        self.central_widget.setObjectName("central_widget")
        self.label.setGeometry(QtCore.QRect(20, 30, 580, 326))
        self.label.setObjectName("label")
        self.pushButton_file.setGeometry(QtCore.QRect(40, 360, 160, 40))
        self.pushButton_file.setObjectName("pushButton_file")
        self.pushButton_play_pause.setGeometry(QtCore.QRect(230, 360, 160, 40))
        self.pushButton_play_pause.setObjectName("pushButton_play_pause")
        self.pushButton_toggle_track.setGeometry(QtCore.QRect(420, 360, 160, 40))
        self.pushButton_toggle_track.setObjectName("pushButton_toggle_track")
        self.label_info.setGeometry(QtCore.QRect(20, 5, 580, 15))
        self.label.setObjectName("label_info")

    # Set up corresponding events.
    def setupEvents(self):
        self.pushButton_file.clicked.connect(self.openFile)
        self.pushButton_play_pause.clicked.connect(self.changeStatus)
        self.pushButton_toggle_track.clicked.connect(self.toggleTracks)

    def retranslateUi(self, MainWindow):
        _translate = QtCore.QCoreApplication.translate
        MainWindow.setWindowTitle(_translate("MainWindow", "OPTICAL FLOW TRACKER"))
        self.label.setText(_translate("MainWindow", 'Click "Open file" button to open a video and view optical flow!'))
        self.label.setAlignment(Qt.AlignCenter)
        font = QtGui.QFont()
        font.setPointSize(19)
        font.setBold(True)
        self.label.setFont(font)
        self.pushButton_file.setText(_translate("MainWindow", "Open file"))
        self.pushButton_play_pause.setText(_translate("MainWindow", "Pause"))
        self.pushButton_toggle_track.setText(_translate("MainWindow", "Toggle tracks"))
        self.label_info.setText(_translate("MainWindow", "Track length:   ; Detect interval:   "
                                                         "; Current number of good features:   ; Current frame: ;"))

    # Choose the video file to open, and process the first frame.
    def openFile(self):
        if self.cap is not None:
            self.cap.release()
        videoName, _ = QFileDialog.getOpenFileName(self.central_widget, "Open", "", "*.mp4;;*.avi")
        self.play_status = True
        self.file_status = True
        self.timer_camera = QTimer()
        self.tracks = []
        self.frame_index = 0
        self.pushButton_play_pause.setText("Pause")
        if videoName != "":
            self.cap = cv2.VideoCapture(videoName)

            # Read first frame of the video
            _, initial_frame = self.cap.read()
            maxDim = max(initial_frame.shape)
            self.scale = resize_dim / maxDim
            initial_frame = cv2.resize(initial_frame, None, fx=self.scale, fy=self.scale)

            self.previousGray = cv2.cvtColor(initial_frame, cv2.COLOR_BGR2GRAY)
            self.mask = np.zeros_like(initial_frame)
            self.timer_camera.start(invoke_sec)
            self.timer_camera.timeout.connect(self.openFrame)

    # Read the frame from the video, and calculate the corresponding optical flow.
    def openFrame(self):
        if self.cap.isOpened():
            self.label_info.setText("Track length = {}; Detect interval = {}; "
                                    "Current good features = {}; Current frame = {}"
                                    .format(track_length, detect_interval, len(self.tracks), self.frame_index))
            ret, self.frame = self.cap.read()
            if ret:
                # Converting to grayscale and resizing.
                gray_frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2GRAY)
                gray_frame = cv2.resize(gray_frame, None, fx=self.scale, fy=self.scale)

                # Resize the output frame.
                self.frame = cv2.resize(self.frame, None, fx=self.scale, fy=self.scale)

                if len(self.tracks) > 0:
                    # Obtain the feature points
                    feature_points = np.float32([tr[-1] for tr in self.tracks]).reshape(-1, 2)

                    # Apply own-implemented optical flow:
                    forward_flow = opticalFlow.lucas_kanade(self.previousGray, gray_frame, feature_points,
                                                            window_size=args.window_size,
                                                            pyramid_level=args.pyramid_level,
                                                            iters=args.iters)
                    backward_flow = opticalFlow.lucas_kanade(gray_frame, self.previousGray, forward_flow,
                                                             window_size=args.window_size,
                                                             pyramid_level=args.pyramid_level,
                                                             iters=args.iters)

                    # Apply built-in optical flow:
                    # forward_flow, _, _ = cv2.calcOpticalFlowPyrLK(self.previousGray, gray_frame, feature_points, None,
                    #                                              **lk_params)
                    # backward_flow, _, _ = cv2.calcOpticalFlowPyrLK(gray_frame, self.previousGray, forward_flow, None,
                    #                                               **lk_params)

                    displacement = abs(feature_points - backward_flow).reshape(-1, 2).max(-1)
                    displacementCheck = displacement < 3
                    new_tracks = []
                    for tr, (x, y), goodCheck in zip(self.tracks, forward_flow.reshape(-1, 2), displacementCheck):

                        # If the displacement check is bad we have errors, we skip this iteration and go to the next one
                        if not goodCheck:
                            continue

                        # We append this position to our tracks list
                        tr.append((x, y))

                        if track_length != 'Infinite':
                            if len(tr) > int(track_length):
                                del tr[0]

                        # We append our tracks
                        new_tracks.append(tr)

                    # Assign our new tracks to our tracks list
                    tracks = new_tracks

                    # For every position for each track we draw a line between them.
                    if self.toggle_track:
                        cv2.polylines(self.frame, [np.int32(tr) for tr in tracks], False, (0, 255, 0))

                if self.frame_index % detect_interval == 0:

                    # Create a mask of pixel values being 255
                    mask = np.zeros_like(gray_frame)
                    mask[:] = 255

                    for x, y in [np.int32(tr[-1]) for tr in self.tracks]:
                        cv2.circle(mask, (x, y), 5, 0, -1)

                    # previous If there is no features, the mask is blank. so we calculate features from scratch
                    features = cv2.goodFeaturesToTrack(gray_frame, mask=mask, **feature_params)

                    # If the features are calculated, we append these to our tracks array to use in the above if loop
                    if features is not None:
                        for x, y in np.float32(features).reshape(-1, 2):
                            self.tracks.append([(x, y)])

                self.frame_index = self.frame_index + 1
                self.previousGray = gray_frame

                # show this image on the corresponding label.
                self.show_image_on_pos()
            else:
                self.cap.release()
                self.timer_camera.stop()

    # Achieve the functionalities of play/pause.
    def changeStatus(self):
        if self.file_status:
            self.play_status = not self.play_status
            if self.play_status:
                self.play()
                self.play_status = True
                self.pushButton_play_pause.setText("Pause")
            else:
                self.pause()
                self.play_status = False
                self.pushButton_play_pause.setText("Play")

    def pause(self):
        self.timer_camera.stop()

    def play(self):
        self.timer_camera = QTimer()
        self.timer_camera.start(invoke_sec)
        self.timer_camera.timeout.connect(self.openFrame)

    # Show the image on the UI.
    def show_image_on_pos(self):
        frame = cv2.cvtColor(self.frame, cv2.COLOR_BGR2RGB)

        height, width, bytesPerComponent = frame.shape
        bytesPerLine = bytesPerComponent * width
        q_image = QImage(frame.data, width, height, bytesPerLine,
                         QImage.Format_RGB888).scaled(self.label.width(), self.label.height())
        self.label.setPixmap(QPixmap.fromImage(q_image))

    # Toggle the tracked lines.
    def toggleTracks(self):
        self.toggle_track = not self.toggle_track


# Program entrance, start application UI.
if __name__ == "__main__":
    app = QtWidgets.QApplication(sys.argv)
    Form = QtWidgets.QMainWindow()
    ui = Ui_MainWindow(Form)
    Form.show()
    sys.exit(app.exec())
